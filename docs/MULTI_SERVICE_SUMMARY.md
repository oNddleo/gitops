# Multi-Service Architecture Summary

Quick reference guide for the VSF-Miniapp multi-service architecture implementation.

---

## Architecture Decision: Shared Base Chart vs Separate Charts

### âœ… Recommended: Shared Base Chart (Implemented)

**Structure:**
```
charts/vsf-miniapp/          # Single shared chart
  â”œâ”€â”€ Chart.yaml
  â”œâ”€â”€ values.yaml            # Common defaults
  â”œâ”€â”€ templates/             # Kubernetes manifests
  â””â”€â”€ ci/                    # Service-specific values
      â”œâ”€â”€ service-a-production.yaml
      â”œâ”€â”€ service-a-staging.yaml
      â”œâ”€â”€ service-a-dev.yaml
      â”œâ”€â”€ service-b-production.yaml
      â””â”€â”€ ...
```

**Advantages:**
- âœ… **DRY**: No duplication of Kubernetes manifests
- âœ… **Consistency**: All services use same deployment patterns
- âœ… **Easy maintenance**: Update one chart, all services benefit
- âœ… **Scalability**: Adding new services is trivial (just add values file)
- âœ… **Governance**: Enforce organizational standards across all services
- âœ… **Testing**: Test once, deploy many

**When to use:**
- Services share similar deployment requirements
- Organization values consistency over flexibility
- Team wants centralized control over infrastructure patterns
- Multiple services from the same platform/team

### âŒ Alternative: Separate Charts Per Service

**Structure:**
```
charts/
  â”œâ”€â”€ service-a/
  â”œâ”€â”€ service-b/
  â””â”€â”€ service-c/
```

**Advantages:**
- âœ… Maximum flexibility per service
- âœ… Independent versioning
- âœ… No coupling between services

**Disadvantages:**
- âŒ Duplication of manifests
- âŒ Inconsistent patterns across services
- âŒ Hard to maintain (N charts to update)
- âŒ Harder to enforce standards

**When to use:**
- Services have vastly different deployment requirements
- Services owned by different teams with different standards
- Services need independent release cycles

---

## Multi-Language Support Strategy

### Language-Agnostic Base Chart + Language-Specific Values

**Base chart provides:**
- Deployment structure
- Service definition
- Security contexts (read-only filesystem, non-root user)
- Health probe endpoints
- Resource limits
- Linkerd injection
- AWS Secrets Manager integration
- HPA, PDB, affinity rules

**Service-specific values override:**
- Container image (Java JRE, Node.js runtime, Python interpreter)
- Port numbers (8080 for Java, 3000 for Node.js, 8000 for Python)
- Environment variables (JAVA_OPTS, NODE_ENV, PYTHONPATH)
- Health probe timing (Java slower startup)
- Resource requirements (Java needs more memory)

### Example Comparison

| Aspect | Java (Service A) | Node.js (Service B) | Python (Service C) |
|--------|------------------|---------------------|-------------------|
| **Port** | 8080 | 3000 | 8000 |
| **Memory Request** | 512Mi | 256Mi | 256Mi |
| **Memory Limit** | 1Gi | 512Mi | 768Mi |
| **Startup Time** | 60s | 15s | 20s |
| **Health Endpoint** | /actuator/health | /health/live | /health/live |
| **Common Config** | All use Linkerd mTLS, AWS Secrets Manager, HPA, PDB |

---

## Component Integration Matrix

| Component | Purpose | Configuration Location | How It Works |
|-----------|---------|----------------------|--------------|
| **Helm Chart** | Package & template | `charts/vsf-miniapp/` | Templates Kubernetes manifests with values |
| **Kustomize** | Environment overlays | `infrastructure/*/overlays/` | Infrastructure components only |
| **ArgoCD** | GitOps operator | `applications/` | Syncs Git state to cluster |
| **Linkerd** | mTLS service mesh | Pod annotation | Automatic sidecar injection |
| **Secrets Store CSI** | Mount secrets | Volume + SecretProviderClass | Mounts AWS secrets as files |
| **Reloader** | Auto-restart on config change | Pod annotation | Watches ConfigMap/Secret changes |
| **Traefik** | Ingress controller | IngressRoute CRD | Routes external traffic to services |

---

## Service Deployment Checklist

When adding a new service (e.g., Service D):

### 1. Create Service-Specific Value Files

```bash
# Create values for each environment
touch charts/vsf-miniapp/ci/service-d-dev.yaml
touch charts/vsf-miniapp/ci/service-d-staging.yaml
touch charts/vsf-miniapp/ci/service-d-production.yaml
```

**Required configurations:**
- [ ] `serviceName: service-d`
- [ ] `language: <java|nodejs|python|...>`
- [ ] `image.repository: YOUR_ECR_REGISTRY/vsf-miniapp-service-d`
- [ ] `service.targetPort: <port>`
- [ ] `resources.requests/limits`
- [ ] `env: []` (language-specific environment variables)
- [ ] `livenessProbe` and `readinessProbe`
- [ ] `secretsManager.objects: []` (AWS secret names)
- [ ] `ingress.hosts: [...]`

### 2. Create ArgoCD Application Manifests

```bash
# Create application manifest for each environment
touch applications/dev/vsf-miniapp-service-d.yaml
touch applications/staging/vsf-miniapp-service-d.yaml
touch applications/production/vsf-miniapp-service-d.yaml
```

**Required configurations:**
- [ ] `metadata.name: vsf-miniapp-service-d-<env>`
- [ ] `metadata.labels.service: service-d`
- [ ] `spec.source.path: charts/vsf-miniapp`
- [ ] `spec.source.helm.valueFiles: [ci/service-d-<env>.yaml]`
- [ ] `spec.destination.namespace: <env>`

### 3. Create AWS Secrets

```bash
# Create secrets in AWS Secrets Manager
aws secretsmanager create-secret \
  --name production/vsf-miniapp/service-d/database \
  --secret-string '{"url":"...","username":"...","password":"..."}' \
  --region us-east-1
```

- [ ] Created secrets in AWS Secrets Manager
- [ ] Created IAM role with IRSA for ServiceAccount
- [ ] Verified IAM policy allows access to secrets

### 4. Commit and Deploy

```bash
git add .
git commit -m "feat: add Service D to vsf-miniapp platform"
git push

# Verify deployment
argocd app sync vsf-miniapp-service-d-production
argocd app wait vsf-miniapp-service-d-production --health --timeout 300
```

- [ ] Committed all changes to Git
- [ ] ArgoCD synced successfully
- [ ] Pods running (check with `kubectl get pods`)
- [ ] Linkerd mTLS enabled (check with `linkerd viz stat`)
- [ ] Secrets mounted (check with `kubectl exec ... ls /mnt/secrets`)
- [ ] Health checks passing
- [ ] Ingress accessible

---

## How Components Work Together

### Scenario: User Request Flow

```
1. External Request
   â†“
2. Traefik Ingress (TLS termination)
   â†“
3. Traefik â†’ Service A Kubernetes Service
   â†“
4. Service (ClusterIP) â†’ Pod
   â†“
5. Linkerd Proxy (mTLS encryption)
   â†“
6. Application Container
   |
   â”œâ”€ Reads ConfigMap (via envFrom)
   â”œâ”€ Reads Secrets (via envFrom from synced K8s Secret)
   â””â”€ Reads Secret Files (via CSI volume mount)
```

### Scenario: Secret Update Flow

```
1. Engineer updates secret in AWS Secrets Manager
   â†“
2. CSI Driver polls every 2 minutes, detects change
   â†“
3. CSI Driver updates mounted files in /mnt/secrets/
   â†“
4. CSI Driver updates synced Kubernetes Secret
   â†“
5. Reloader watches Secret, detects change
   â†“
6. Reloader triggers rolling update of Deployment
   â†“
7. New pods start with updated secrets
```

### Scenario: Code Deployment Flow

```
1. Developer pushes code to main branch
   â†“
2. GitHub Actions CI triggers
   â†“
3. CI builds Docker image, pushes to ECR
   â†“
4. CI updates charts/vsf-miniapp/ci/service-a-production.yaml with new tag
   â†“
5. CI commits change back to Git
   â†“
6. ArgoCD detects Git change
   â†“
7. ArgoCD syncs new image tag to cluster
   â†“
8. Kubernetes performs rolling update
   â†“
9. Linkerd proxy automatically injected in new pods
   â†“
10. New pods come online with mTLS enabled
```

---

## Linkerd mTLS Implementation

### How mTLS Works

```
Service A â†’ Service B Communication:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pod: Service A    â”‚           â”‚   Pod: Service B    â”‚
â”‚                     â”‚           â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ App Containerâ”‚   â”‚           â”‚  â”‚ App Containerâ”‚   â”‚
â”‚  â”‚  (Java)      â”‚   â”‚           â”‚  â”‚  (Node.js)   â”‚   â”‚
â”‚  â”‚              â”‚   â”‚           â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ http://      â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”     â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ service-b    â”‚   â”‚     â”‚     â”‚  â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â†“           â”‚     â”‚     â”‚         â–²           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Linkerd    â”‚   â”‚     â”‚     â”‚  â”‚   Linkerd    â”‚   â”‚
â”‚  â”‚   Proxy      â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¼â”€â–¶â”‚   Proxy      â”‚   â”‚
â”‚  â”‚  (mTLS)      â”‚   â”‚   HTTPS   â”‚  â”‚  (mTLS)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   + Cert  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Application code: Plain HTTP
Network traffic: Encrypted HTTPS with mutual TLS
```

### Configuration Required

**In Helm values (already configured):**
```yaml
podAnnotations:
  linkerd.io/inject: enabled
  config.linkerd.io/skip-outbound-ports: "5432,3306,6379,27017"  # Skip databases
```

**That's it!** Linkerd handles the rest automatically.

### Verification Commands

```bash
# Check if Linkerd proxy is injected
kubectl get pod <pod-name> -n production -o jsonpath='{.spec.containers[*].name}'
# Expected: service-a linkerd-proxy

# Check mTLS status
linkerd viz stat deployment/service-a -n production
# Expected: SECURED with 100% success rate

# View live traffic
linkerd viz tap deployment/service-a -n production
# Shows real-time requests with mTLS status

# View service topology
linkerd viz edges deployment -n production
# Shows which services communicate with each other
```

---

## AWS Secrets Manager Integration

### Architecture

```
AWS Secrets Manager
       â†“
    IRSA (IAM Role for Service Account)
       â†“
SecretProviderClass (defines what to fetch)
       â†“
CSI Driver (mounts secrets as files)
       â†“
Kubernetes Secret (synced for env vars)
       â†“
Application (reads via env vars or file mount)
```

### Files Involved

1. **AWS Secret** (created via AWS CLI/Console)
2. **IAM Role** (created via Terraform/IRSA)
3. **ServiceAccount** (in Helm chart)
   ```yaml
   serviceAccount:
     annotations:
       eks.amazonaws.com/role-arn: "arn:aws:iam::ACCOUNT_ID:role/..."
   ```
4. **SecretProviderClass** (in Helm chart template)
   ```yaml
   spec:
     provider: aws
     parameters:
       objects: |
         - objectName: "production/vsf-miniapp/service-a/database"
   ```
5. **Volume Mount** (in Deployment template)
   ```yaml
   volumes:
     - name: secrets-store
       csi:
         driver: secrets-store.csi.k8s.io
         volumeAttributes:
           secretProviderClass: service-a-secrets
   ```

### Secret Rotation

**Automatic (every 2 minutes):**
1. CSI driver polls AWS Secrets Manager
2. Detects changes and updates mounted files
3. Updates synced Kubernetes Secret
4. Reloader triggers rolling update
5. New pods get updated secrets

**Manual trigger:**
```bash
# Update secret in AWS
aws secretsmanager update-secret --secret-id ... --secret-string '{...}'

# Wait 2 minutes or force pod restart
kubectl rollout restart deployment/service-a -n production
```

---

## Troubleshooting Quick Reference

| Symptom | Likely Cause | Check Command | Solution |
|---------|-------------|---------------|----------|
| Pods stuck in Init | Secret mount failure | `kubectl describe pod <pod>` | Verify IRSA, SecretProviderClass |
| No mTLS | Proxy not injected | `kubectl get pod -o yaml \| grep linkerd` | Add `linkerd.io/inject: enabled` |
| Secret not found | IRSA permissions | `kubectl run aws-cli ... secretsmanager get-secret-value` | Fix IAM policy |
| App won't sync | Helm values error | `argocd app get <app>` | Check Helm syntax, fix values file |
| 502 Bad Gateway | Service not ready | `kubectl get pods -l service=...` | Check readiness probe |
| ConfigMap changes ignored | Reloader not working | `kubectl logs -l app=reloader` | Verify Reloader annotation |

---

## Performance Characteristics

| Metric | Dev | Staging | Production |
|--------|-----|---------|-----------|
| **Replicas** | 1-2 | 2-5 | 3-20 (HPA) |
| **Resource Requests (Java)** | 100m CPU, 256Mi RAM | 200m CPU, 512Mi RAM | 250m CPU, 512Mi RAM |
| **Resource Requests (Node.js)** | 50m CPU, 128Mi RAM | 100m CPU, 256Mi RAM | 100m CPU, 256Mi RAM |
| **Sync Frequency (ArgoCD)** | 3 minutes | 3 minutes | 3 minutes |
| **Secret Rotation (CSI)** | 2 minutes | 2 minutes | 2 minutes |
| **Deployment Time** | ~2 min | ~3 min | ~5 min (rolling) |

---

## File Organization Reference

```
gitops/
â”œâ”€â”€ charts/vsf-miniapp/                    # Shared Helm chart
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml                        # Common defaults
â”‚   â”œâ”€â”€ templates/                         # Kubernetes manifest templates
â”‚   â”‚   â”œâ”€â”€ _helpers.tpl
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â”œâ”€â”€ serviceaccount.yaml
â”‚   â”‚   â”œâ”€â”€ secretproviderclass.yaml      # AWS Secrets Manager CSI
â”‚   â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”‚   â”œâ”€â”€ ingressroute.yaml             # Traefik ingress
â”‚   â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”‚   â””â”€â”€ servicemonitor.yaml           # Prometheus metrics
â”‚   â””â”€â”€ ci/                                # Service-specific values
â”‚       â”œâ”€â”€ service-a-dev.yaml
â”‚       â”œâ”€â”€ service-a-staging.yaml
â”‚       â”œâ”€â”€ service-a-production.yaml
â”‚       â”œâ”€â”€ service-b-dev.yaml
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ applications/                          # ArgoCD Application manifests
â”‚   â”œâ”€â”€ infrastructure/                    # Infrastructure apps
â”‚   â”‚   â”œâ”€â”€ 00-project.yaml
â”‚   â”‚   â”œâ”€â”€ argocd-self-managed.yaml
â”‚   â”‚   â”œâ”€â”€ linkerd-crds.yaml
â”‚   â”‚   â”œâ”€â”€ linkerd-control-plane.yaml
â”‚   â”‚   â”œâ”€â”€ linkerd-viz.yaml
â”‚   â”‚   â”œâ”€â”€ secrets-store-csi-driver.yaml
â”‚   â”‚   â”œâ”€â”€ secrets-store-csi-driver-provider-aws.yaml
â”‚   â”‚   â”œâ”€â”€ traefik.yaml
â”‚   â”‚   â””â”€â”€ reloader.yaml
â”‚   â”œâ”€â”€ app-of-apps-dev.yaml              # Dev environment App of Apps
â”‚   â”œâ”€â”€ app-of-apps-staging.yaml          # Staging environment App of Apps
â”‚   â”œâ”€â”€ app-of-apps-production.yaml       # Production environment App of Apps
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ vsf-miniapp-service-a.yaml
â”‚   â”‚   â””â”€â”€ vsf-miniapp-service-b.yaml
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ vsf-miniapp-service-a.yaml
â”‚   â”‚   â””â”€â”€ vsf-miniapp-service-b.yaml
â”‚   â””â”€â”€ production/
â”‚       â”œâ”€â”€ vsf-miniapp-service-a.yaml
â”‚       â””â”€â”€ vsf-miniapp-service-b.yaml
â”‚
â”œâ”€â”€ infrastructure/                        # Infrastructure Kustomize + Helm
â”‚   â”œâ”€â”€ argocd/
â”‚   â”œâ”€â”€ linkerd/
â”‚   â”œâ”€â”€ linkerd-viz/
â”‚   â”œâ”€â”€ traefik/
â”‚   â”œâ”€â”€ secrets-store-csi/
â”‚   â””â”€â”€ reloader/
â”‚
â”œâ”€â”€ bootstrap/                             # Bootstrap scripts
â”‚   â”œâ”€â”€ install.sh
â”‚   â”œâ”€â”€ argocd-namespace.yaml
â”‚   â””â”€â”€ root-app.yaml                     # Root App of Apps
â”‚
â””â”€â”€ .github/workflows/                     # CI/CD pipelines
    â”œâ”€â”€ ci-build-deploy.yaml
    â”œâ”€â”€ pr-validation.yaml
    â””â”€â”€ infrastructure-sync.yaml
```

---

## Next Steps

1. **Review** existing implementation in your repository
2. **Read** ARCHITECTURE_GUIDE.md for detailed explanations
3. **Follow** IMPLEMENTATION_GUIDE.md for step-by-step migration
4. **Reference** examples in `examples/multi-service/` directory
5. **Deploy** to dev environment first
6. **Verify** Linkerd mTLS and AWS Secrets Manager integration
7. **Promote** to staging and production

Your platform is **production-ready** with excellent foundations! ğŸš€
